{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44a020f-90b5-41d6-86aa-ba2aa03fff33",
   "metadata": {},
   "source": [
    "# Activity 6\n",
    "For this activity, let's consider the previous classification problem:\n",
    "\n",
    "## Predicting Customer Churn in a Telecom Company\n",
    "### Problem Overview\n",
    "In this classification problem, the goal is to predict whether a customer will churn (leave) or stay with a telecom company based on several features such as customer demographics, service usage, and account information.\n",
    "\n",
    "- Class label: Churn (1 = Yes, 0 = No)\n",
    "\n",
    "    - 1: Customer has churned.\n",
    "    - 0: Customer has stayed.\n",
    "    \n",
    "- Features\n",
    "    - Customer ID: Unique identifier for each customer.\n",
    "    - Gender: Whether the customer is male or female.\n",
    "    - Age: Age of the customer.\n",
    "    - Tenure: Number of months the customer has been with the company.\n",
    "    - Service Plan: Type of service plan (e.g., Basic, Premium).\n",
    "    - Monthly Charges: Monthly bill of the customer.\n",
    "    - Total Charges: Total amount billed to the customer.\n",
    "    - Internet Service: Whether the customer has internet service (Yes/No).\n",
    "    - Tech Support: Whether the customer has tech support (Yes/No).\n",
    "    - Paperless Billing: Whether the customer opts for paperless billing (Yes/No).\n",
    "    - Payment Method: Payment method (e.g., Bank Transfer, Credit Card).\n",
    "    - Contract Type: Contract type (e.g., Month-to-month, One year, Two year).\n",
    "    - Phone Service: Whether the customer has phone service (Yes/No).\n",
    "    - Multiple Lines: Whether the customer has multiple lines (Yes/No).\n",
    "    \n",
    "### Classification Models:\n",
    "- K-Nearest Neighbors (KNN) density estimation\n",
    "    - For density estimation, KNN can be used to estimate the probability density of a data point by looking at the nearest neighbors of that point.\n",
    "- Support Vector Machine (SVM) with linear kernel for hard and soft margin\n",
    "    - Linear SVM is the simplest form of SVM where the data is assumed to be linearly separable. It finds the hyperplane that best separates the data into two classes.\n",
    "    - Hard Margin SVM aims to find the hyperplane that perfectly separates the classes with no margin violations (i.e., no points on the wrong side of the hyperplane). This is only possible when the data is perfectly linearly separable.\n",
    "    - Soft margin SVM allows some margin violations (misclassifications) but penalizes them through a cost parameter C. The parameter C controls the trade-off between maximizing the margin and minimizing classification errors. A large C value makes the model more sensitive to misclassifications, while a smaller C allows more misclassifications but results in a wider margin.\n",
    "- Non-Linear SVM\n",
    "    - When the data is not linearly separable, we use non-linear kernels such as RBF (Radial Basis Function), Polynomial, and Sigmoid. These kernels transform the data into a higher-dimensional space where a linear separation is possible.\n",
    "    - RBF is a popular non-linear kernel that maps the data into a higher-dimensional space using a Gaussian function.\n",
    "    - A polynomial kernel uses polynomial functions to map the data into a higher-dimensional space. This can be useful when the data has non-linear relationships.\n",
    "    - The sigmoid kernel uses a sigmoid function to map the data to a higher-dimensional space. It's less commonly used, but still an option.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "We will use the same dataset we used in Activity 4. This dataset contains information for over 7000 customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f9d25-1cf2-4808-8af7-1f478f81f0b6",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a25418-a76c-4155-8089-064ccee480d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23004872-fe09-4e74-91e1-90605c97ed39",
   "metadata": {},
   "source": [
    "## Load data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3155363-d88e-493f-8db5-6502306f01e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2694c2-7dbc-4e66-bd21-06e45351ec78",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984f9db-2467-4d79-b77d-2d47a431c98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns (e.g., CustomerID, Churn, etc.) from numerical operations\n",
    "df_cleaned = df.drop(columns=['customerID'])\n",
    "\n",
    "# Identify numerical columns (you can also use df.select_dtypes() if needed)\n",
    "numerical_columns = df_cleaned.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Identify categorical columns (object, category)\n",
    "categorical_columns = df_cleaned.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Impute missing values with the mean for numerical columns\n",
    "df_cleaned[numerical_columns] = df_cleaned[numerical_columns].fillna(df_cleaned[numerical_columns].mean())\n",
    "# Impute missing values for categorical columns with the mode (most frequent value)\n",
    "for column in categorical_columns:\n",
    "    mode_value = df_cleaned[column].mode()[0]  # Get the most frequent value\n",
    "    df_cleaned[column] = df_cleaned[column].fillna(mode_value)\n",
    "    \n",
    "# Convert 'TotalCharges' to numeric (force errors to NaN)\n",
    "df_cleaned['TotalCharges'] = pd.to_numeric(df_cleaned['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Refill any NaN values in 'TotalCharges' (if any)\n",
    "df_cleaned['TotalCharges'] = df_cleaned['TotalCharges'].fillna(df_cleaned['TotalCharges'].mean())\n",
    "\n",
    "# encoding for categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df_cleaned.drop('Churn', axis=1)\n",
    "y = df_cleaned['Churn']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (important for KNN and Naive Bayes)\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Plot histograms for all features\n",
    "X[numerical_columns].hist(bins=10, figsize=(10, 10), layout=(3, 4))\n",
    "plt.suptitle('Distribution of Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pairwise relationships between features (using pairplot)\n",
    "sns.pairplot(df_cleaned, hue='Churn', vars=numerical_columns, markers=[\"o\", \"s\"])\n",
    "plt.suptitle('Pairwise Relationships', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr = df_cleaned.corr()  # Compute the correlation matrix\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Class Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_cleaned[numerical_columns], y=df_cleaned['Churn'])\n",
    "plt.title('Class Distribution', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Identifying Outliers (Boxplots)\n",
    "plt.figure(figsize=(24, 8))\n",
    "\n",
    "# Create boxplots for all features\n",
    "for i, feature in enumerate(X.columns):\n",
    "    plt.subplot(6, 4, i+1)\n",
    "    sns.boxplot(x=y, y=feature, data=df)\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0f93a-4df1-4e55-80d3-31a179d2d711",
   "metadata": {},
   "source": [
    "## Train Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5055091-6b95-4721-a30a-a691ddf541fd",
   "metadata": {},
   "source": [
    "### Visualizing the difference between hard and soft margin SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ae3fa-2d57-4fe1-b327-a308fa2aef89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_2D = X_train[['tenure', 'MonthlyCharges']]\n",
    "X_test_2D = X_test[['tenure', 'MonthlyCharges']]\n",
    "\n",
    "#Train a linear support vector machine with hard margin\n",
    "hard_margin_svm_2D = SVC(kernel='linear', C=1e10)\n",
    "hard_margin_svm_2D.fit(X_train_2D,y_train)\n",
    "#Train a linear support vector machine with soft margin\n",
    "soft_margin_svm_2D = SVC(kernel='linear', C=1)\n",
    "soft_margin_svm_2D.fit(X_train_2D,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac76437-3630-4cab-9195-fc28250f5a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a plot to show their differences in the decision boundaries\n",
    "x_min, x_max = X_train_2D[['tenure']].min() - 1,x_min, x_max = X_train_2D[['tenure']].max + 1 \n",
    "y_min, y_max = X_train_2D[['MonthlyCharges']].min() - 1,y_min, y_max = X_train_2D[['MonthlyCharges']].max + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min,x_max, 100), np.linespace(y_min, y_max,100))\n",
    "\n",
    "#plot hard margin SVM decision boundary\n",
    "plt.subplot(1,2,1)\n",
    "z_hard = hard_margin_svm_2D.predict(np.c_p[xx.ravel(),yy.ravel()])\n",
    "z_hard = z_hard.reshape(xx.shape)\n",
    "plt.contourf(xx,yy,z_hard, alpha=0.8)\n",
    "plt.scatter(X_train_2D[['tenure']],X_train_2D[['MonthlyCharges']],c=y_train, edgecolors='k', marker = 'o', s=30)\n",
    "plt.title(\"Soft Margin SVm\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad979e6-4b5c-44f1-93c2-1d7cf6609c96",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347f8c4-2dad-44e9-bf01-86a0f0929b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the linear SVM\n",
    "linear_svm = SVC(kernel='linear')\n",
    "linear_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d257b-2fc3-4bc9-a787-1bfb9abfd647",
   "metadata": {},
   "source": [
    "### Non-linear SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e8720-e7d4-4e04-aadb-25fa16ad2ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the RBF SVM\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "\n",
    "#Train the polynomial SVM\n",
    "poly_svm = SVC(kernel='poly')\n",
    "poly_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4ab6c-064b-4e25-b10c-8bc05ef63138",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning in RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4d6c0-7493-4c13-bafd-ab3920cb5319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameter tuning in RBF \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_grid = {'c': [0.1, 1],\n",
    "              'gamma': ['scale', 'auto'],\n",
    "              'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "random_search = RandomizedSearchCV('svm', param_distribution=param_grid, n_iter=10, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "#Best parameters and results\n",
    "best_svm = random_search.best_params_\n",
    "print(\"Best parameters found:\" , best_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10bc0a76-85fc-4a3d-a1e5-92f31a959a93",
   "metadata": {},
   "source": [
    "### KNN Density Estimation for Classification\n",
    "KNN is generally used as a classifier by considering the majority vote of the k nearest neighbors. Howeever, for density estimation, we look at how the KNN algorithm can be used to estimate the likelihood of the target variable for a given data point.\n",
    "In a KNN density estimation setup, we would typically be interested in:\n",
    "- **Probability Estimation**: The probability of a class for a given point can be approximated by the proportion of neighbors that belong to that class. For example, if 3 out of the 5 nearest neighbors of a data point belong to class 1 (churn), the estimated probability of that point belonging to the class 1 is 0.6.\n",
    "\n",
    "However, for simplicity, we will use KNN as a classifier directly since it fits with the task more naturally and use it predict via probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee44821-b3e4-4a6f-87f1-db26bbe3d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9652bd9-6b8c-4fd3-a183-4a14f0d902bb",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cab62-fcd0-4ab2-8add-7eb67bf74ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the parameters for param grid of KNN\n",
    "param_grid_knn = {'n_neighbors': [3,5,7,9,11],\n",
    "                  'weights': ['uniform', 'distance'],\n",
    "                  'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search_knn = GridSearchCV(KNeighborsClassifier(),param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "best_knn = grid_search_knn.best_params_\n",
    "print(\"Best Parameters Found:\", best_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84cfdfe-ad15-461e-9ac9-5927a5e6cbae",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e0ae9-da8e-4914-8af7-ad3d4a91bc06",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b5bf5-da61-479f-b7e9-2bc224d00f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lvsm_tr = linear_svm.predict(X_train)\n",
    "y_pred_lvsm_tt = linear_svm.predict(X_test)\n",
    "\n",
    "y_pred_rbf_tr = rbf_svm.predict(X_train)\n",
    "y_pred_rbf_tt = rbf_svm.predict(X_test)\n",
    "\n",
    "y_pred_poly_tr = poly_svm.predict(X_train)\n",
    "y_pred_poly_tt = poly_svm.predict(X_test)\n",
    "\n",
    "y_pred_bsvm_tr = best_svm.predict(X_train)\n",
    "y_pred_bsvm_tt = best_svm.predict(X_test)\n",
    "\n",
    "y_prob_knn_tr = knn.predict_proba(X_train)\n",
    "y_prob_knn_tt = knn.predict_proba(X_test)\n",
    "\n",
    "y_pred_knn_tr = (y_prob_knn_tr[:,1] > 0.5).astype(int)\n",
    "y_pred_knn_tt = (y_prob_knn_tt[:,1] > 0.5).astype(int)\n",
    "\n",
    "y_prob_bknn_tr = best_knn.predict_proba(X_train)\n",
    "y_prob_bknn_tt = best_knn.predict_proba(X_test)\n",
    "\n",
    "y_pred_bknn_tr = (y_prob_bknn_tr[:,1] > 0.5).astype(int)\n",
    "y_pred_bknn_tt = (y_prob_bknn_tt[:,1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd7a72-7efc-42a9-802a-91915efbb8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a59bf4-d500-47fd-93e2-b10d498ecba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {}\n",
    "accuracy['linear SVM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fb3c6-0eb8-4a6c-8730-213744159279",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d5147-c369-4f87-a779-95be9a373ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame(accuracy, index=['Training Set', 'Testin Set']).T\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7ff78-d9f3-4442-a137-302fcb3fc1d3",
   "metadata": {},
   "source": [
    "### Conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc712e51-5c69-42bb-a1b6-32abffef2355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
