{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, classification_report, roc_curve)\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier, BaggingClassifier, \n",
    "                              GradientBoostingClassifier, AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed and creat a synthetic dataset (of 20,000) with the features provided in the project description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_records = 20000\n",
    "\n",
    "#Gernerate data for each feature\n",
    "data = {\n",
    "    \"Age\": np.random.randint(18,65,size=n_records),\n",
    "    \"Gender\": np.random.choice([\"Male\",\"Female\"],size=n_records),\n",
    "    \"Marital_Status\": np.random.choice([\"Single\",\"Married\",\"Divorced\"], size=n_records),\n",
    "    \"Education_Level\": np.random.choice([\"High School\", \"Bachelor\",\"Master\",\"PhD\"], size=n_records),\n",
    "    \"Job_Role\": np.random.choice([\"Sales Executive\", \"Research Scientist\", \"Manager\"], size=n_records),\n",
    "    \"Department\": np.random.choice([\"Sales\",\"Marketing\",\"HR\",\"IT\"], size=n_records),\n",
    "\n",
    "    \"Job_Satisfaction\": np.random.randint(1,6, size=n_records),\n",
    "    \"Work_Life_Balance\": np.random.randint(1,6,size=n_records),\n",
    "    \"Number_Of_Companies_Worked\": np.random.randint(1,10, size=n_records),\n",
    "    \"Distance_From_Home\": np.random.uniform(1,50, size=n_records).round(2),\n",
    "    \"Overtime\": np.random.choice([\"Yes\",\"No\"],size=n_records),\n",
    "    \"Job_Involvement\":np.random.randint(1,6, size=n_records),\n",
    "\n",
    "    \"Salary\": np.random.uniform(30000, 150000, size=n_records),\n",
    "    \"Hourly_Rate\": np.random.uniform(15,100, size=n_records).round(2),\n",
    "    \"Stock_Option_Level\": np.random.randint(0,5, size=n_records),\n",
    "    \"Bonuses\": np.random.choice([\"Yes\",\"No\"], size=n_records),\n",
    "\n",
    "    \"Years_At_Company\": np.random.randint(0,40, size=n_records),\n",
    "    \"Training_Times_Last_Year\": np.random.randint(0,10,size=n_records),\n",
    "    \"Job_Level\": np.random.randint(1,6, size=n_records),\n",
    "    \"Employee_Recognition\": np.random.choice([\"Yes\", \"No\"], size=n_records),\n",
    "\n",
    "    \"Performance_Rating\": np.random.randint(1,6, size=n_records),\n",
    "    \"Absenteeism\": np.random.randint(0,20, size=n_records),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Department</th>\n",
       "      <th>Job_Satisfaction</th>\n",
       "      <th>Work_Life_Balance</th>\n",
       "      <th>Number_Of_Companies_Worked</th>\n",
       "      <th>Distance_From_Home</th>\n",
       "      <th>...</th>\n",
       "      <th>Hourly_Rate</th>\n",
       "      <th>Stock_Option_Level</th>\n",
       "      <th>Bonuses</th>\n",
       "      <th>Years_At_Company</th>\n",
       "      <th>Training_Times_Last_Year</th>\n",
       "      <th>Job_Level</th>\n",
       "      <th>Employee_Recognition</th>\n",
       "      <th>Performance_Rating</th>\n",
       "      <th>Absenteeism</th>\n",
       "      <th>Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>Male</td>\n",
       "      <td>Married</td>\n",
       "      <td>Master</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>IT</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24.32</td>\n",
       "      <td>...</td>\n",
       "      <td>37.95</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>Male</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Master</td>\n",
       "      <td>Manager</td>\n",
       "      <td>HR</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26.74</td>\n",
       "      <td>...</td>\n",
       "      <td>52.35</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>Male</td>\n",
       "      <td>Married</td>\n",
       "      <td>Master</td>\n",
       "      <td>Manager</td>\n",
       "      <td>IT</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>22.91</td>\n",
       "      <td>...</td>\n",
       "      <td>37.21</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>Female</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Master</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>Sales</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>39.29</td>\n",
       "      <td>...</td>\n",
       "      <td>93.75</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>Male</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>HR</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>31.32</td>\n",
       "      <td>...</td>\n",
       "      <td>27.63</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender Marital_Status Education_Level            Job_Role Department  \\\n",
       "0   56    Male        Married          Master  Research Scientist         IT   \n",
       "1   46    Male       Divorced          Master             Manager         HR   \n",
       "2   32    Male        Married          Master             Manager         IT   \n",
       "3   60  Female       Divorced          Master     Sales Executive      Sales   \n",
       "4   25    Male       Divorced        Bachelor     Sales Executive         HR   \n",
       "\n",
       "   Job_Satisfaction  Work_Life_Balance  Number_Of_Companies_Worked  \\\n",
       "0                 2                  3                           3   \n",
       "1                 4                  2                           1   \n",
       "2                 2                  2                           8   \n",
       "3                 4                  1                           7   \n",
       "4                 1                  4                           3   \n",
       "\n",
       "   Distance_From_Home  ... Hourly_Rate  Stock_Option_Level  Bonuses  \\\n",
       "0               24.32  ...       37.95                   4      Yes   \n",
       "1               26.74  ...       52.35                   2      Yes   \n",
       "2               22.91  ...       37.21                   1      Yes   \n",
       "3               39.29  ...       93.75                   0       No   \n",
       "4               31.32  ...       27.63                   3      Yes   \n",
       "\n",
       "   Years_At_Company  Training_Times_Last_Year Job_Level  Employee_Recognition  \\\n",
       "0                31                         1         2                    No   \n",
       "1                28                         4         2                   Yes   \n",
       "2                25                         1         1                   Yes   \n",
       "3                28                         2         2                   Yes   \n",
       "4                 9                         3         3                    No   \n",
       "\n",
       "   Performance_Rating  Absenteeism Attrition  \n",
       "0                   3           12         0  \n",
       "1                   1           17         0  \n",
       "2                   3            2         0  \n",
       "3                   5            9         0  \n",
       "4                   1           14         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creat the DataFram\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Generate the target variables with calss imbalance of 10% attrition rate\n",
    "df[\"Attrition\"] = np.where(np.random.rand(n_records) < 0.1,1,0)\n",
    "\n",
    "#Display the first few rows\n",
    "rows, cols = df.shape\n",
    "print(f\"Number of Rows: {rows}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                           0\n",
      "Gender                        0\n",
      "Marital_Status                0\n",
      "Education_Level               0\n",
      "Job_Role                      0\n",
      "Department                    0\n",
      "Job_Satisfaction              0\n",
      "Work_Life_Balance             0\n",
      "Number_Of_Companies_Worked    0\n",
      "Distance_From_Home            0\n",
      "Overtime                      0\n",
      "Job_Involvement               0\n",
      "Salary                        0\n",
      "Hourly_Rate                   0\n",
      "Stock_Option_Level            0\n",
      "Bonuses                       0\n",
      "Years_At_Company              0\n",
      "Training_Times_Last_Year      0\n",
      "Job_Level                     0\n",
      "Employee_Recognition          0\n",
      "Performance_Rating            0\n",
      "Absenteeism                   0\n",
      "Attrition                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill numerical columns with median\n",
    "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "#Fill categorical columns with mode\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode all categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify numerical columns (excluding the target variable)\n",
    "num_cols = [col for col in df_encoded.columns if df_encoded[col].dtype in [\"int64\", \"float64\"] and col != \"Attrition\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded[num_cols] = scaler.fit_transform(df_encoded[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat Models and compare the performance using three different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate features and target\n",
    "X = df_encoded.drop(\"Attrition\", axis=1)\n",
    "y = df_encoded[\"Attrition\"]\n",
    "\n",
    "#Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate the ratio of negative to positive calsses in y_train\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "\n",
    "#Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, class_weight={0:1, 1:10}),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric = \"logloss\", random_state=42, scale_pos_weight=scale_pos_weight)\n",
    "}\n",
    "\n",
    "# Function to evaluate and print model performance\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    print(f\"----- {name} -----\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    if y_pred_prob is not None:\n",
    "        print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1296 candidates, totalling 3888 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:31:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'scale_pos_weight': 8.467455621301776, 'subsample': 0.8}\n",
      "Training Random Forest with SMOTE resampled data...\n",
      "----- Random Forest -----\n",
      "Accuracy: 0.858\n",
      "Precision: 0.10526315789473684\n",
      "Recall: 0.04784688995215311\n",
      "F1 Score: 0.06578947368421052\n",
      "ROC-AUC: 0.5062727246012092\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92      3582\n",
      "           1       0.11      0.05      0.07       418\n",
      "\n",
      "    accuracy                           0.86      4000\n",
      "   macro avg       0.50      0.50      0.49      4000\n",
      "weighted avg       0.81      0.86      0.83      4000\n",
      "\n",
      "\n",
      "\n",
      "Training Gradient Boosting with SMOTE resampled data...\n",
      "----- Gradient Boosting -----\n",
      "Accuracy: 0.845\n",
      "Precision: 0.0860655737704918\n",
      "Recall: 0.050239234449760764\n",
      "F1 Score: 0.0634441087613293\n",
      "ROC-AUC: 0.5216533224335393\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92      3582\n",
      "           1       0.09      0.05      0.06       418\n",
      "\n",
      "    accuracy                           0.84      4000\n",
      "   macro avg       0.49      0.49      0.49      4000\n",
      "weighted avg       0.81      0.84      0.83      4000\n",
      "\n",
      "\n",
      "\n",
      "Training XGBoost with SMOTE resampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:31:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- XGBoost -----\n",
      "Accuracy: 0.78875\n",
      "Precision: 0.09177820267686425\n",
      "Recall: 0.11483253588516747\n",
      "F1 Score: 0.10201912858660998\n",
      "ROC-AUC: 0.4988418968847427\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      3582\n",
      "           1       0.09      0.11      0.10       418\n",
      "\n",
      "    accuracy                           0.79      4000\n",
      "   macro avg       0.49      0.49      0.49      4000\n",
      "weighted avg       0.81      0.79      0.80      4000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Train each model and evaluate\\nfor name, model in models.items():\\n    print(f\"Training {name}...\")\\n    model.fit(X_train, y_train)\\n    evaluate_model(name, model, X_test, y_test)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resample training data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define a parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'scale_pos_weight': [scale_pos_weight, scale_pos_weight * 2]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring='f1', cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Best parameters for XGBoost:\", grid_search.best_params_)\n",
    "\n",
    "# Replace the XGBoost model in our models dictionary with the tuned estimator\n",
    "models[\"XGBoost\"] = grid_search.best_estimator_\n",
    "\n",
    "# Now train your models with the resampled data\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} with SMOTE resampled data...\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    evaluate_model(name, model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "----- Random Forest -----\n",
      "Accuracy: 0.8955\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "ROC-AUC: 0.492635292357588\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94      3582\n",
      "           1       0.00      0.00      0.00       418\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.45      0.50      0.47      4000\n",
      "weighted avg       0.80      0.90      0.85      4000\n",
      "\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Gradient Boosting -----\n",
      "Accuracy: 0.8955\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "ROC-AUC: 0.5315840232528939\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94      3582\n",
      "           1       0.00      0.00      0.00       418\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.45      0.50      0.47      4000\n",
      "weighted avg       0.80      0.90      0.85      4000\n",
      "\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:47:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- XGBoost -----\n",
      "Accuracy: 0.8705\n",
      "Precision: 0.08333333333333333\n",
      "Recall: 0.023923444976076555\n",
      "F1 Score: 0.03717472118959108\n",
      "ROC-AUC: 0.49795695649967003\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      3582\n",
      "           1       0.08      0.02      0.04       418\n",
      "\n",
      "    accuracy                           0.87      4000\n",
      "   macro avg       0.49      0.50      0.48      4000\n",
      "weighted avg       0.81      0.87      0.84      4000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train each model and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_model(name, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Prediction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Bonuses\n- Department\n- Education_Level\n- Employee_Recognition\n- Gender\n- ...\nFeature names seen at fit time, yet now missing:\n- Bonuses_Yes\n- Department_IT\n- Department_Marketing\n- Department_Sales\n- Education_Level_High School\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 62\u001b[0m\n\u001b[0;32m     36\u001b[0m sample_employee \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m35\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbsenteeism\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     59\u001b[0m }\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Make a prediction for the sample employee\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m prediction, probability \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_employee_attrition\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_employee\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessing_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeave\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStay\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction for sample employee:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m, in \u001b[0;36mpredict_employee_attrition\u001b[1;34m(employee_features, model, pipeline)\u001b[0m\n\u001b[0;32m     21\u001b[0m df_input \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([employee_features])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing pipeline to the new data\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m X_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Make predictions using the trained model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_processed)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:903\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    901\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter():\n\u001b[1;32m--> 903\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xt\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Bonuses\n- Department\n- Education_Level\n- Employee_Recognition\n- Gender\n- ...\nFeature names seen at fit time, yet now missing:\n- Bonuses_Yes\n- Department_IT\n- Department_Marketing\n- Department_Sales\n- Education_Level_High School\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Choose final model (e.g., the tuned XGBoost) for predictions\n",
    "final_model = models[\"XGBoost\"]\n",
    "\n",
    "# ------------------------\n",
    "# 10. Prediction Function for New Employee Data\n",
    "# ------------------------\n",
    "def predict_employee_attrition(employee_features, model, pipeline):\n",
    "    \"\"\"\n",
    "    Predicts whether an employee will leave based on input features.\n",
    "    \n",
    "    Parameters:\n",
    "        employee_features (dict): Dictionary of employee feature values.\n",
    "        model: The trained classifier (final_model).\n",
    "        pipeline: The preprocessing pipeline (preprocessing_pipeline).\n",
    "    \n",
    "    Returns:\n",
    "        prediction (int): 1 if the employee is predicted to leave, 0 otherwise.\n",
    "        probability (float): The probability of leaving.\n",
    "    \"\"\"\n",
    "    # Convert the input dictionary to a DataFrame\n",
    "    df_input = pd.DataFrame([employee_features])\n",
    "    \n",
    "    # Apply the preprocessing pipeline to the new data\n",
    "    X_processed = pipeline.transform(df_input)\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    prediction = model.predict(X_processed)\n",
    "    prediction_prob = model.predict_proba(X_processed)[:, 1]\n",
    "    \n",
    "    return int(prediction[0]), float(prediction_prob[0])\n",
    "\n",
    "# ------------------------\n",
    "# 11. Test the Prediction Function\n",
    "# ------------------------\n",
    "# Define a sample employee's features (adjust these to match your original feature set)\n",
    "sample_employee = {\n",
    "    \"Age\": 35,\n",
    "    \"Gender\": \"Male\",\n",
    "    \"Marital_Status\": \"Married\",\n",
    "    \"Education_Level\": \"Bachelor\",\n",
    "    \"Job_Role\": \"Sales Executive\",\n",
    "    \"Department\": \"Sales\",\n",
    "    \"Job_Satisfaction\": 3,\n",
    "    \"Work_Life_Balance\": 3,\n",
    "    \"Number_Of_Companies_Worked\": 2,\n",
    "    \"Distance_From_Home\": 10.0,\n",
    "    \"Overtime\": \"No\",\n",
    "    \"Job_Involvement\": 3,\n",
    "    \"Salary\": 60000,\n",
    "    \"Hourly_Rate\": 35,\n",
    "    \"Stock_Option_Level\": 1,\n",
    "    \"Bonuses\": \"Yes\",\n",
    "    \"Years_At_Company\": 5,\n",
    "    \"Training_Times_Last_Year\": 2,\n",
    "    \"Job_Level\": 3,\n",
    "    \"Employee_Recognition\": \"No\",\n",
    "    \"Performance_Rating\": 4,\n",
    "    \"Absenteeism\": 2\n",
    "}\n",
    "\n",
    "# Make a prediction for the sample employee\n",
    "prediction, probability = predict_employee_attrition(sample_employee, final_model, preprocessing_pipeline)\n",
    "result = \"Leave\" if prediction == 1 else \"Stay\"\n",
    "print(\"Prediction for sample employee:\", result)\n",
    "print(\"Probability of leaving: {:.2f}\".format(probability))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
