{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353e07a6-6b83-4a4d-a861-4768d79d4e80",
   "metadata": {},
   "source": [
    "## **In-Class Activity: Customer Segmentation using Mall Customer Data**\n",
    "\n",
    "### **Dataset**  \n",
    "Use the **Mall Customer Segmentation Dataset** (available on [Kaggle](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python)).\n",
    "\n",
    "It contains:\n",
    "- Customer ID\n",
    "- Gender\n",
    "- Age\n",
    "- Annual Income (k$)\n",
    "- Spending Score (1–100)\n",
    "\n",
    "### **Activity Objective**\n",
    "\n",
    "Apply and compare the performance of **K-Means**, **Hierarchical Clustering**, and **DBSCAN** to discover customer segments based on their income and spending behavior.\n",
    "\n",
    "\n",
    "### **Instructions**\n",
    "\n",
    "1. **Load and Explore the Dataset**  \n",
    "   \n",
    "2. **Apply Clustering Algorithms**  \n",
    "   Implement the following:\n",
    "   - **K-Means** (try different `K` values; use elbow method)\n",
    "   - **K-Mediods** \n",
    "   - **Hierarchical Clustering** (use dendrogram to choose number of clusters)\n",
    "   - **DBSCAN** (tune `eps` and `minPts`)\n",
    "\n",
    "3. **Visualize the Clusters**  \n",
    "   - Use scatter plots to visualize results of each algorithm.\n",
    "\n",
    "4. **Evaluate the Clustering Results**  \n",
    "   - Use **Silhouette Score** for internal validation.\n",
    "   - Discuss the differences in how each algorithm handles cluster shapes and outliers.\n",
    "\n",
    "\n",
    "### **Guiding Questions**\n",
    "\n",
    "You might want to reflect on:\n",
    "- Which algorithm performed best for this dataset and why?\n",
    "- How does DBSCAN treat outliers differently from K-Means?\n",
    "- What are the limitations of using only `Annual Income` and `Spending Score`?\n",
    "- How would feature scaling affect DBSCAN’s performance?\n",
    "\n",
    "### **Expected Learning Outcome**\n",
    "\n",
    "By the end of this activity, you will:\n",
    "- Understand the practical behavior of each algorithm\n",
    "- Compare clustering shapes: spherical (K-Means), K-Mediods, nested (Hierarchical), and arbitrary (DBSCAN)\n",
    "- Develop intuition for parameter tuning and internal validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1e9e4-bc3d-4030-8612-3112b6e007f6",
   "metadata": {},
   "source": [
    "# **Clustering Algorithms Overview**\n",
    "\n",
    "## 1. **K-Means**\n",
    "- **Type**: Centroid-based\n",
    "- **Working**: Divides data into `k` clusters by minimizing the within-cluster sum of squared distances (inertia).\n",
    "- **Strengths**: Fast, simple, effective on spherical and equally sized clusters.\n",
    "- **Limitations**: Assumes clusters are isotropic; sensitive to initial centroid placement and outliers.\n",
    "\n",
    "## 2. **K-Medoids**\n",
    "A medoid is a representative object in a cluster whose average dissimilarity to all the other objects in the cluster is minimal. Unlike a centroid (which is an average point that may not exist in the dataset), a medoid is always an actual data point from the dataset.\n",
    "- **Type**: Partitioning (similar to K-Means)\n",
    "- **Working**: Instead of centroids, it uses actual data points (medoids) as centers. It minimizes the sum of dissimilarities.\n",
    "- **Strengths**: More robust to outliers and noise than K-Means.\n",
    "- **Limitations**: Computationally heavier than K-Means.\n",
    "\n",
    "#### 3. **Hierarchical Clustering (Agglomerative)**\n",
    "- **Type**: Hierarchical\n",
    "- **Working**: Bottom-up approach where each point starts as a cluster and merges based on distance criteria (e.g., Ward linkage).\n",
    "- **Strengths**: No need to pre-specify number of clusters; visual via dendrogram.\n",
    "- **Limitations**: Computationally expensive for large datasets; hard to undo once merged.\n",
    "\n",
    "#### 4. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "- **Type**: Density-based\n",
    "- **Working**: Groups together points that are closely packed and marks low-density regions as noise.\n",
    "- **Strengths**: Can find arbitrarily shaped clusters; handles noise well.\n",
    "- **Limitations**: Sensitive to `eps` and `min_samples`; struggles with varying density.\n",
    "\n",
    "#### 5. **BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)**\n",
    "- **Type**: Hierarchical + Centroid-based hybrid\n",
    "- **Working**: Builds a CF tree and clusters incrementally and hierarchically.\n",
    "- **Strengths**: Efficient for very large datasets; scalable.\n",
    "- **Limitations**: Performance degrades with non-spherical clusters or poor threshold selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520492c2-57a9-4148-b09a-53a5919926b1",
   "metadata": {},
   "source": [
    "### **Internal Validation Metrics**\n",
    "Internal metrics assess the quality of the clustering based solely on the input data and the resulting labels, without relying on any ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Silhouette Score**\n",
    "\n",
    "- **What it measures:**  \n",
    "  How well each data point fits within its own cluster vs. other clusters.\n",
    "\n",
    "- **Range:**  \n",
    "  -1 (poor clustering) to +1 (ideal clustering), with ~0 indicating overlapping clusters.\n",
    "\n",
    "- **Formula (for a point):**  \n",
    "  $$\n",
    "  s = \\frac{b - a}{\\max(a, b)}\n",
    "  $$\n",
    "  - $ a $ = average intra-cluster distance (within the same cluster)  \n",
    "  - $ b $ = average nearest-cluster distance (to the nearest cluster that the point is not part of)\n",
    "\n",
    "- **Use case:**  \n",
    "  Useful to evaluate how compact and well-separated the clusters are.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Calinski-Harabasz Index (Variance Ratio Criterion)**\n",
    "\n",
    "- **What it measures:**  \n",
    "  Ratio of between-cluster dispersion to within-cluster dispersion.\n",
    "\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  CH = \\frac{\\text{Between-cluster variance}}{\\text{Within-cluster variance}} \\times \\frac{(n - k)}{(k - 1)}\n",
    "  $$\n",
    "  - $ n $ = number of samples, $ k $ = number of clusters\n",
    "\n",
    "- **Interpretation:**  \n",
    "  Higher is better; a high value indicates well-separated and compact clusters.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Davies-Bouldin Index**\n",
    "\n",
    "- **What it measures:**  \n",
    "  Average similarity between each cluster and its most similar cluster (based on cluster diameter and distance).\n",
    "\n",
    "- **Range:**  \n",
    "  $\\geq 0$; **lower is better**.\n",
    "\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\ne i} \\left( \\frac{s_i + s_j}{d_{ij}} \\right)\n",
    "  $$\n",
    "  - $ s_i $ = intra-cluster distance of cluster $ i $  \n",
    "  - $ d_{ij} $ = inter-cluster distance between clusters $ i $ and $ j $\n",
    "\n",
    "---\n",
    "\n",
    "### **External Validation Metric**\n",
    "External metrics compare the clustering result with a known ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Adjusted Rand Index (ARI)**\n",
    "\n",
    "- **What it measures:**  \n",
    "  Agreement between the clustering and the true class labels, adjusted for chance.\n",
    "\n",
    "- **Range:**  \n",
    "  From -1 to 1  \n",
    "  - 1: Perfect match  \n",
    "  - 0: Random labeling  \n",
    "  - < 0: Worse than random\n",
    "\n",
    "- **Use case:**  \n",
    "  Effective when ground truth labels are available and you want to evaluate clustering accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| **Metric**                | **Ideal Value** | **Interpretation**                              |\n",
    "|---------------------------|------------------|--------------------------------------------------|\n",
    "| Silhouette Score          | Close to 1       | High cohesion + separation                      |\n",
    "| Calinski-Harabasz Index   | High             | High between-cluster & low within-cluster var.  |\n",
    "| Davies-Bouldin Index      | Low              | Better inter-cluster separation                 |\n",
    "| Adjusted Rand Index (ARI) | Close to 1       | Matches ground truth labeling                   |"
   ]
  },
  {
   "cell_type": "raw",
   "id": "924a659f-c7d1-4c0e-be31-60630187cecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb80581b-d552-4a24-8ad6-45aba7c88e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Customer Segmentation: Full Clustering Pipeline with Evaluation and Comparison (With and Without Hyperparameter Tuning)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, Birch\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd62a8a-fc4a-4442-b04c-f695d2b78b79",
   "metadata": {},
   "source": [
    "# Initial Setup with Annual Income and Spending Behavior only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1966e9-3b69-4c02-9157-dca8ca179a26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a169b-8113-4e32-828f-aff98bc05cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "248b7def-042a-4e53-b5e1-1eab8970d005",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. BASELINE MODEL EVALUATION (No tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb7393-d86a-4554-878b-39e39ceec247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8f50e7-6cac-4c67-8333-1a4cb503a97f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KMeans Baseline + Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533c51d-2de0-4943-ba24-7b0351c46b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8934d4-ede9-47cd-b638-2c84d3744c79",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hierarchical Clustering + Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd9093-121f-4a68-b6cc-831034a3efb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "434bff4a-5d54-4c73-af42-482dc5804570",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320a406-629d-4bb0-b80e-0ee90722ef89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c312ccf1-ef5f-4388-87a6-bb7b7784de5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BIRCH Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0da66e-3548-4890-baba-0ebdb91874b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9680725-a312-46f3-b59d-116b7864e665",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8c54f-1907-4092-9d87-8ecbdfd78864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "838a29f4-b373-4240-9e3e-569c0473c772",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. COMPARATIVE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1d5af-30a2-44a0-9518-50cdf8502a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f501d190-ece8-49b1-a3d4-c3efce802a56",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What happens when all predictors are considered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b950873-124e-4633-b4e6-3f971fb5d7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90ba6a-54c8-4a88-8600-a17e58c152d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
